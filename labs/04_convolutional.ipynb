{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üëÅÔ∏è Convolutional Neural Networks (CNNs)\n",
    "\n",
    "[Deep Learning](https://dsai.units.it/index.php/courses/deep-learning/) Course @ [UniTS](https://portale.units.it/en), Spring 2024\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/emaballarin/deeplearning-units/blob/main/labs/04_convolutional.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>  <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/emaballarin/deeplearning-units/blob/main/labs/04_convolutional.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ],
   "id": "e159d90f57e1219f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a staple architecture in the field of (*deep-learning-based*) Computer Vision, and the go-to choice for visual tasks, *e.g.* image classification, segmentation and recognition among the many.\n",
    "\n",
    "The main advantages of CNNs lie in the use of a *Convolutional Layer*, exhibiting a **position-invariant** inductive bias and very limited learnable parameter count compared to equivalent FC models. This allows CNNs to learn hierarchical features from the input data, and to generalize better to unseen data."
   ],
   "id": "51f7840957c3639"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The building blocks of a CNN\n",
    "\n",
    "The essential building blocks of convolutional neural networks are the following.\n",
    "\n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "The basic building block of a CNN is - indeed - the **convolutional layer**, available in PyTorch as `torch.nn.Conv<s>d`, where `<s>` represents the number of **spatial dimensions** of our data. *I.e.:*\n",
    "\n",
    "* `Conv1d` for $1$-dimensional sequences. *Example*: audio. Audio is organized as a sequence of a given length (the single spatial dimension), where each single value in this sequence represent the intensity/amplitude of the signal for a given sampled time point. Audio data can be organized in multiple **channels** (e.g., stereo data has 2 channels). The convolution operation is represented by a one-dimensional kernel;\n",
    "\n",
    "* `Conv2d` for 2 dimensional data, like images (where the two dimensions are **spatial** dimensions, *e.g.* height and width);\n",
    "* \n",
    "* `Conv3d` for 3 dimensional data. *Example*: a 3D reconstruction obtained from a 2D image. A convolution in that domain might equate to sliding a cubic kernel along all three dimensions.\n",
    "\n",
    "#### (Some) parameters for constructors\n",
    "\n",
    "```python\n",
    "Conv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0)\n",
    "```\n",
    "* `in_channels`: the number of channels (planes) of the incoming data;\n",
    "* `out_channels`: the number of channels (planes) of the desired output data, *i.e.*, the number of convolutions that are performed;\n",
    "* `kernel_size`: the kernel size of each convolution. An int $k$ is interpreted as the tuple $(k, k)$ (i.e., a square kernel); for a rectangular kernel, pass a tuple explicitly.\n",
    "* `stride`: the step size used when moving the kernel on the input data, along each dimension. An int $s$ is interpreted as the tuple $(s, s)$ (i.e., an isotropic kernel); for an anisotropic stride, pass a tuple explicitly;\n",
    "* `padding`: if set to $>0$, the incoming image is enlarged with `padding` rows and columns of zeros (unless otherwise specified).\n",
    "\n",
    "To visualize these and other parameters and how they affect the convolution operation, you can have a look at [this page](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "\n",
    "Note that a convolution does **NOT** require to specify a spatial dimension as input/output, as convolution is oblivious to these factors!\n",
    "\n",
    "To better visualize how convolutions work with multi-channel data, have a look at [this](https://www.coursera.org/lecture/convolutional-neural-networks/convolutions-over-volume-ctQZz) short video by Andrew Ng."
   ],
   "id": "f7351a2842e8dca8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preliminary infrastucture setup\n",
    "\n",
    "Nothing new here, just the usual cloud-aware setup..."
   ],
   "id": "46bcdf752ed14a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "FOLDERNAME: str = \"deeplearning_units_2024\"\n",
    "try:\n",
    "    if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(BASEPATH := \"/content/drive\")\n",
    "        os.makedirs(FULLPATH := BASEPATH + \"/MyDrive/\" + FOLDERNAME, exist_ok=True)\n",
    "    elif os.getenv(\"KAGGLE_CONTAINER_NAME\"):\n",
    "        os.makedirs(FULLPATH := \"/kaggle/working/\" + FOLDERNAME, exist_ok=True)\n",
    "    else:\n",
    "        os.makedirs(FULLPATH := \"./\" + FOLDERNAME, exist_ok=True)\n",
    "    os.chdir(FULLPATH)\n",
    "except (ModuleNotFoundError, FileExistsError, FileNotFoundError):\n",
    "    pass"
   ],
   "id": "659e133448d51960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The imports for the day",
   "id": "24cb146cb783d55d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv\n",
    "\n",
    "from typing import Union, Callable\n",
    "from tqdm import trange\n",
    "from typing import List\n",
    "\n",
    "from ebtorch.nn.utils import eval_model_on_test\n",
    "from ebtorch.nn.utils import extract_conv_filters, show_filters\n",
    "\n",
    "from safetensors.torch import save_model, load_model"
   ],
   "id": "d3aa85b8523ce8ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### A simple example",
   "id": "d656a56c55fc1f8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conv_layer_example: nn.Conv2d = th.nn.Conv2d(\n",
    "    in_channels=3, out_channels=32, kernel_size=3\n",
    ")\n",
    "print(\n",
    "    \"Parameters of convolution\\n\",\n",
    "    \"\\tWeights\\n\",\n",
    "    conv_layer_example.weight.shape,\n",
    "    \"\\n\\tBias\\n\",\n",
    "    conv_layer_example.bias.shape,\n",
    ")"
   ],
   "id": "863d4ffdbf1336d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Conv2d is applied independently of the input spatial dimension\n",
    "y: Tensor = conv_layer_example(th.rand(1, 3, 10, 10))\n",
    "print(\"Output shape\\n\", y.shape)"
   ],
   "id": "726dc90eef2838e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z: Tensor = conv_layer_example(th.rand(1, 3, 6, 6))\n",
    "print(\"Output shape\\n\", z.shape)"
   ],
   "id": "a55154bdaa2cb037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pooling layers\n",
    "\n",
    "*Pooling layers* are essentially convolutions **without trainable kernels**. For each overlap between the image and the kernel, they output the maximum ($\\rightarrow$ _maxpooling_) or the average ($\\rightarrow$ _avgpooling_), *etc...* of the image in that specific region.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)\n",
    "\n",
    "#### (Some) parameters for constructors\n",
    "\n",
    "```MaxPool2d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0)```\n",
    "\n",
    "Notice that now we do not have nthe need to pass input or output channels as parameter, because *Max/Avg/...* Pooling act independently on each channel, so `in_channels=out_channels`!\n",
    "\n",
    "#### Adaptive Pooling\n",
    "\n",
    "Adaptive (*Max/Avg/...*) Pooling is still a pooling layer, parameterised by the option to specify the **desired spatial dimension** of the output instead of parameters like *kernel size*, *padding*, *etc., ...*.\n",
    "\n",
    "PyTorch works out by itself the parameters required in order for the pooling to produce an output of the desired size.\n",
    "\n",
    "The most common application of this layer is (maybe) when operating the channel-wise average pooling at the end of a cascade of convolutional layers. In this case, we specify a fixed size of $(1,1)$, such that PyTorch will essentially operate an average of each whole channel."
   ],
   "id": "4512c8e51d78311f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aavgpool: nn.AdaptiveAvgPool2d = th.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "print(aavgpool(th.rand(1, 3, 32, 32)).shape)"
   ],
   "id": "b3df3336b460861b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Flattening\n",
    "\n",
    "The last crucial *layer* in a CNN is the **flattening** *layer* (more of an *operation*, which induces a **reshaping** of the data from a multi-dimensional tensor to a 1D tensor. This is required in order to feed it to a final fully connected layer."
   ],
   "id": "aff35c9eb8187d73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Before flattening\\n\", th.rand(1, 3, 32, 32).shape)\n",
    "print(\"After flattening\\n\", th.nn.Flatten()(th.rand(1, 3, 32, 32)).shape)"
   ],
   "id": "3c13d324039d2b78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a CNN\n",
    "\n",
    "The *typical model training* pipeline will be exactly the same as in the previous labs (*e.g.* that on gradient-based training of a FCN on the MNIST dataset), with the only differences being the **dataset** and **model architecture**."
   ],
   "id": "53f8d20c4810d009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "DEVICE_AUTODETECT: bool = True\n",
    "TRAIN_BATCH_SIZE: int = 64\n",
    "TEST_BATCH_SIZE: int = 1000\n",
    "EPOCHS: int = 15\n",
    "CRITERION: Union[th.nn.Module, Callable[[th.Tensor], th.Tensor]] = (\n",
    "    th.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    ")\n",
    "EVAL_CRITERION: Union[th.nn.Module, Callable[[th.Tensor], th.Tensor]] = (\n",
    "    th.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    ")\n",
    "LR: float = 2e-3"
   ],
   "id": "b6ffde3e9b9951de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Device setup\n",
    "device: th.device = th.device(\n",
    "    \"cuda\" if th.cuda.is_available() and DEVICE_AUTODETECT else \"cpu\"\n",
    ")"
   ],
   "id": "d31bb7daf0f09fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### The data",
   "id": "cba55bb30be06144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cifarten_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.makedirs(\"./data/\", exist_ok=True)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, transform=cifarten_transforms, download=True\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, transform=cifarten_transforms, download=True\n",
    ")\n",
    "\n",
    "train_loader: DataLoader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader: DataLoader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False\n",
    ")"
   ],
   "id": "ef7fe94cdefdf2ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's have a look at the data we just loaded.",
   "id": "4eec1fc21df05b8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def imshow(img) -> None:\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, axes=(1, 2, 0)))\n",
    "\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = train_loader.__iter__()\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "# Show grid of images\n",
    "imshow(tv.utils.make_grid(images[: 8 * 4], normalize=True))"
   ],
   "id": "f51ee864c28d5d3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see the associated labels\n",
    "print(f\"Labels: {labels[:8*4]}\")"
   ],
   "id": "d422513abc11647f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see the shape of the images\n",
    "print(f\"Images shape: {images.shape}\")"
   ],
   "id": "7c99bac730b293fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyCNN(th.nn.Module):\n",
    "    def __init__(self, cls_out: int = 10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = th.nn.Sequential(\n",
    "            th.nn.Conv2d(\n",
    "                in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=0\n",
    "            ),\n",
    "            th.nn.Mish(),\n",
    "            th.nn.MaxPool2d(kernel_size=2),\n",
    "            th.nn.Dropout(p=0.2),\n",
    "            th.nn.Conv2d(\n",
    "                in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            th.nn.Mish(),\n",
    "            th.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            th.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.head = th.nn.Linear(392, cls_out)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.head(self.conv(x))"
   ],
   "id": "229f729161b889d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model: MyCNN = MyCNN().to(device)\n",
    "model.train()"
   ],
   "id": "940053e3a4ce86c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's define the optimizer\n",
    "optimizer: th.optim.Optimizer = th.optim.Adam(\n",
    "    params=model.parameters(), lr=LR, weight_decay=0\n",
    ")"
   ],
   "id": "76b2d9bfdc5bb27f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_losses: List[float] = []\n",
    "eval_acc: List[float] = []\n",
    "test_acc: List[float] = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in trange(EPOCHS, desc=\"Training epoch\"):\n",
    "\n",
    "    model.train()  # Remember to set the model in training mode before actual training\n",
    "\n",
    "    # Loop over data\n",
    "    for batch_idx, batched_datapoint in enumerate(train_loader):\n",
    "\n",
    "        x, y = batched_datapoint\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass + loss computation\n",
    "        yhat = model(x)\n",
    "        loss = CRITERION(yhat, y)\n",
    "\n",
    "        # Zero-out past gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log the loss and accuracy on the training set...\n",
    "    num_elem: int = 0\n",
    "    trackingmetric: float = 0\n",
    "    trackingcorrect: int = 0\n",
    "\n",
    "    model.eval()  # Remember to set the model in evaluation mode before evaluating it\n",
    "\n",
    "    # Since we are just evaluating the model, we don't need to compute gradients\n",
    "    with th.no_grad():\n",
    "        # ... by looping over training data again\n",
    "        for _, batched_datapoint_e in enumerate(train_loader):\n",
    "            x_e, y_e = batched_datapoint_e\n",
    "            x_e, y_e = x_e.to(device), y_e.to(device)\n",
    "            modeltarget_e = model(x_e)\n",
    "            ypred_e = th.argmax(modeltarget_e, dim=1, keepdim=True)\n",
    "            trackingmetric += EVAL_CRITERION(modeltarget_e, y_e).item()\n",
    "            trackingcorrect += ypred_e.eq(y_e.view_as(ypred_e)).sum().item()\n",
    "            num_elem += x_e.shape[0]\n",
    "        eval_losses.append(trackingmetric / num_elem)\n",
    "        eval_acc.append(trackingcorrect / num_elem)\n",
    "\n",
    "    # Let's ignore for now what the next line does... üôà\n",
    "    test_acc.append(\n",
    "        eval_model_on_test(\n",
    "            model, True, test_loader, device, th.nn.CrossEntropyLoss(), False\n",
    "        )\n",
    "    )"
   ],
   "id": "128dff9abee6501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_color = \"tab:red\"\n",
    "acc_color = \"tab:blue\"\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\", color=loss_color)\n",
    "ax1.plot(eval_losses, color=loss_color)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=loss_color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Accuracy\", color=acc_color)\n",
    "ax2.plot(eval_acc, color=acc_color)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=acc_color)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.title(\"Training loss and accuracy\")\n",
    "plt.show()"
   ],
   "id": "bee68c277b235d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Final training loss: {eval_losses[-1]}\")\n",
    "print(f\"Final training accuracy: {eval_acc[-1]}\")"
   ],
   "id": "bb3f111206055e8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's plot the accuracy on the train vs test set\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.plot(eval_acc, label=\"Train\", color=\"tab:blue\")\n",
    "ax.plot(test_acc, label=\"Test\", color=\"tab:orange\")\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Training vs. Test accuracy\")\n",
    "plt.show()"
   ],
   "id": "44cf3f07f503eebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Final test accuracy: {test_acc[-1]}\")",
   "id": "fe5fa658ed9e4540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our (relatively simple) CNN is definitely performing better than what a similar MLP would have obtained in such task (within the same training length).\n",
    "\n",
    "Somehow counterintuitively, the number of parameters of our current model is much smaller than of that hypothetical MLP.\n",
    "\n",
    "What really makes CNNs so good at CV tasks **is not the model complexity** (number of parameters), **but the inductive bias they produce**!"
   ],
   "id": "2bf67996d3f674d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_params_num(net: nn.Module) -> int:\n",
    "    return sum(map(th.numel, net.parameters()))"
   ],
   "id": "a47c3c21d1ba7529",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_params_num(model)",
   "id": "d4ebee87133a1f0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_model(model, \"cifar10_model.safetensors\")\n",
    "save_model(model.conv, \"cifar10_featurizer.safetensors\")"
   ],
   "id": "1dbe2ad40ba68ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transfer Learning\n",
    "\n",
    "In Deep Learning practice, people rarely train **large** models from scratch (*i.e.* from randomly initialized weights). This is especially true in Computer Vision (or Language Modelling!), where *pre-trained* weights for many *standard* models are openly available.\n",
    "\n",
    "In the usual setting, when facing a Computer Vision task (unless for some reason you want to use a customized architecture!), you load a pre-trained model and **fine-tune** it.\n",
    "\n",
    "Fine-tuning can follow different paths: one possibility is to freeze all the layers of the network excluding the last (the classification head), another is to train the whole end-to-end classifier starting from pre-trained weights.\n",
    "\n",
    "### A feature-freezing example\n",
    "\n",
    "As an example, we will try to re-use the features learned by our previously trained network (which was trained on CIFAR-10) on the much more complex CIFAR-100 dataset. The results will be surely unsatisfactory, but the point here is to show how to proceed."
   ],
   "id": "f5ba60ff1c60f727"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cifarhundred_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.makedirs(\"./data/\", exist_ok=True)\n",
    "\n",
    "train_dataset_new = datasets.CIFAR100(\n",
    "    root=\"./data\", train=True, transform=cifarhundred_transforms, download=True\n",
    ")\n",
    "test_dataset_new = datasets.CIFAR100(\n",
    "    root=\"./data\", train=False, transform=cifarhundred_transforms, download=True\n",
    ")\n",
    "\n",
    "train_loader_new: DataLoader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader_new: DataLoader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False\n",
    ")"
   ],
   "id": "15d22b131a75312c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def imshow(img) -> None:\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, axes=(1, 2, 0)))\n",
    "\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = train_loader.__iter__()\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "# Show grid of images\n",
    "imshow(tv.utils.make_grid(images[: 8 * 4], normalize=True))"
   ],
   "id": "12d54fbd909e0b4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see the associated labels\n",
    "print(f\"Labels: {labels[:8*4]}\")"
   ],
   "id": "e47124672d1758ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's see the shape of the images\n",
    "print(f\"Images shape: {images.shape}\")"
   ],
   "id": "5b84ec0dd31882b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_new: MyCNN = MyCNN(cls_out=100).to(device)\n",
    "model_new.train()"
   ],
   "id": "c020dca79af67f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def zero_grad_otf(base_model: nn.Module, learnable_name: str = \"head\") -> None:\n",
    "    for name, param in base_model.named_parameters():\n",
    "        if learnable_name not in name:\n",
    "            param.grad = None\n",
    "            param.requires_grad = False"
   ],
   "id": "d4e7fdaa61d7f48e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_model(model_new.conv, \"cifar10_featurizer.safetensors\")",
   "id": "273dc074d3b327c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "zero_grad_otf(model_new, \"head\")",
   "id": "5c078e7d00e66729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's define the optimizer\n",
    "optimizer_new: th.optim.Optimizer = th.optim.Adam(\n",
    "    params=model_new.parameters(), lr=LR, weight_decay=0\n",
    ")"
   ],
   "id": "1d6d092eac01eb42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_losses_new: List[float] = []\n",
    "eval_acc_new: List[float] = []\n",
    "test_acc_new: List[float] = []\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Training epoch\"):\n",
    "\n",
    "    model_new.train()  # Remember to set the model_new in training mode before actual training\n",
    "\n",
    "    # Loop over data\n",
    "    for batch_idx, batched_datapoint in enumerate(train_loader_new):\n",
    "\n",
    "        x, y = batched_datapoint\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        zero_grad_otf(model_new, \"head\")  # Ensure that only the head is trained\n",
    "        yhat = model_new(x)\n",
    "        loss = CRITERION(yhat, y)\n",
    "\n",
    "        optimizer_new.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_new.step()\n",
    "\n",
    "    num_elem: int = 0\n",
    "    trackingmetric: float = 0\n",
    "    trackingcorrect: int = 0\n",
    "\n",
    "    model_new.eval()\n",
    "\n",
    "    with th.no_grad():\n",
    "        for _, batched_datapoint_e in enumerate(train_loader_new):\n",
    "            x_e, y_e = batched_datapoint_e\n",
    "            x_e, y_e = x_e.to(device), y_e.to(device)\n",
    "            model_target_e = model_new(x_e)\n",
    "            ypred_e = th.argmax(model_target_e, dim=1, keepdim=True)\n",
    "            trackingmetric += EVAL_CRITERION(model_target_e, y_e).item()\n",
    "            trackingcorrect += ypred_e.eq(y_e.view_as(ypred_e)).sum().item()\n",
    "            num_elem += x_e.shape[0]\n",
    "        eval_losses_new.append(trackingmetric / num_elem)\n",
    "        eval_acc_new.append(trackingcorrect / num_elem)\n",
    "\n",
    "    test_acc_new.append(\n",
    "        eval_model_on_test(\n",
    "            model_new, True, test_loader_new, device, th.nn.CrossEntropyLoss(), False\n",
    "        )\n",
    "    )"
   ],
   "id": "22658c31f5986eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_color = \"tab:red\"\n",
    "acc_color = \"tab:blue\"\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\", color=loss_color)\n",
    "ax1.plot(eval_losses_new, color=loss_color)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=loss_color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Accuracy\", color=acc_color)\n",
    "ax2.plot(eval_acc_new, color=acc_color)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=acc_color)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.title(\"Training loss and accuracy\")\n",
    "plt.show()"
   ],
   "id": "104a4c22bac05adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Final training loss: {eval_losses_new[-1]}\")\n",
    "print(f\"Final training accuracy: {eval_acc_new[-1]}\")"
   ],
   "id": "718726258e860bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's plot the accuracy on the train vs test set\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.plot(eval_acc_new, label=\"Train\", color=\"tab:blue\")\n",
    "ax.plot(test_acc_new, label=\"Test\", color=\"tab:orange\")\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Training vs. Test accuracy\")\n",
    "plt.show()"
   ],
   "id": "9dba13cb47f48ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Final test accuracy: {test_acc_new[-1]}\")",
   "id": "572f4b9b330b7f96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Visualising the features\n",
    "\n",
    "Let's have a look at the filters learned by the first convolutional layer of our model."
   ],
   "id": "5c730ed88fa82d7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conv1_filters: th.Tensor = extract_conv_filters(model_new.conv[0])\n",
    "show_filters(conv1_filters)\n",
    "conv1_filters.shape"
   ],
   "id": "3336b40ecb900f51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning\n",
    "\n",
    "The procedure required by network *fine-tuning* is indeed simpler that that required for *feature freezing*. We just need to load the weights obtained from the previously-learnt (whole!) model and train the network end-to-end."
   ],
   "id": "cde195a9d7462b4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transposed Convolutions\n",
    "\n",
    "Transpose Convolutions follow a working principle that is very similar to that of *normal* Convolutions, but they are used to generate an image starting from some *features* instead of the reverse.\n",
    "\n",
    "The parameters to construct a Transposed Convolutional layer are exactly the same as for Convolutional layers:\n",
    "\n",
    "```python\n",
    "ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0)\n",
    "```\n",
    "However, note that the meaning of some of these parameters may be counterintuitive, since its meaning will be **reversed** *w.r.t.* that used for *standard* convolutions!\n",
    "\n",
    "[Example with `stride=2`, `kernel_size=3`, `padding=0`.](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides_transposed.gif)\n",
    "\n",
    "[Example with `stride=2`, `kernel_size=3`, `padding=1`.](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_transposed.gif)\n",
    "\n",
    "\n",
    "Transposed Convolutions can be used to build *autoencoders* and *GANs* - among the many - instrumental in order to build **generative models** capable of image-data generation."
   ],
   "id": "45ccac12c8a39adc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
